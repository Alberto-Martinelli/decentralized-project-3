{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Redundancy and Distributed Computing Workshop - TD3-A**  \n",
    "In this notebook we explore the transition from local computation to decentralized prediction models. We will develop multiple predictive models on a selected dataset, integrate them into an API, and introduce a **consensus mechanism** with a **slashing protocol** to enhance reliability in a decentralized setting.  \n",
    "\n",
    "_Authors_: Alessia SARRITZU, Alberto MARTINELLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Selection \n",
    "For this workshop, we decided to work with the **Titanic dataset**. It provides information about passengers aboard the Titanic, including their demographics, ticket class, and whether they survived the disaster.  \n",
    "\n",
    "**Core Features**\n",
    "- **Survived (`survived`)** – Target variable indicating whether the passenger survived (1 = Yes, 0 = No).  \n",
    "- **Passenger Class (`pclass`)** – The ticket class (1 = First, 2 = Second, 3 = Third).  \n",
    "- **Sex (`sex`)** – Gender of the passenger (`male` or `female`).  \n",
    "- **Age (`age`)** – Age of the passenger (may contain missing values).  \n",
    "- **Siblings/Spouses aboard (`sibsp`)** – Number of siblings/spouses traveling with the passenger.  \n",
    "- **Parents/Children aboard (`parch`)** – Number of parents/children traveling with the passenger.  \n",
    "- **Fare (`fare`)** – The fare paid for the ticket.  \n",
    "- **Embarked (`embarked`)** – Port of embarkation (`C` = Cherbourg, `Q` = Queenstown, `S` = Southampton).  \n",
    "\n",
    "**Additional Features**\n",
    "- **Class (`class`)** – Alternative representation of `pclass` (`First`, `Second`, `Third`).  \n",
    "- **Who (`who`)** – Categorizes passengers as `man`, `woman`, or `child` based on age and gender.  \n",
    "- **Adult Male (`adult_male`)** – Boolean flag (`True` = adult male, `False` = otherwise).  \n",
    "- **Deck (`deck`)** – Deck location of the cabin (many missing values).  \n",
    "- **Embark Town (`embark_town`)** – Full name of the embarkation town (`Southampton`, `Cherbourg`, `Queenstown`).  \n",
    "- **Alive (`alive`)** – String representation of survival status (`yes` or `no`).  \n",
    "- **Alone (`alone`)** – Boolean indicating if the passenger was traveling alone (`True` = alone, `False` = had family aboard).  \n",
    "\n",
    "Next, we will load and preprocess the dataset before developing individual predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and Preprocessing\n",
    "\n",
    "Before training our machine learning models, it is essential to perform **data cleaning and preprocessing** to ensure that the dataset is free of inconsistencies, missing values, and irrelevant features. This step enhances model performance and ensures reliable predictions.  \n",
    "\n",
    "In this section, we:  \n",
    "1. **Load the Titanic dataset** and examine its structure.  \n",
    "2. **Analyze the class distribution** to understand the balance between survivors and non-survivors.  \n",
    "3. **Handle missing values**, specifically by dropping the `deck` column (which has excessive missing data) and removing any remaining incomplete rows.  \n",
    "4. **Prepare features for machine learning**, including encoding categorical variables (`sex`, `pclass`, and `embarked`) and standardizing numerical features (`age`, `sibsp`, `parch`, `fare`).  \n",
    "5. **Split the dataset into training and testing sets**, ensuring our models are evaluated fairly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 15 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   survived     891 non-null    int64   \n",
      " 1   pclass       891 non-null    int64   \n",
      " 2   sex          891 non-null    object  \n",
      " 3   age          714 non-null    float64 \n",
      " 4   sibsp        891 non-null    int64   \n",
      " 5   parch        891 non-null    int64   \n",
      " 6   fare         891 non-null    float64 \n",
      " 7   embarked     889 non-null    object  \n",
      " 8   class        891 non-null    category\n",
      " 9   who          891 non-null    object  \n",
      " 10  adult_male   891 non-null    bool    \n",
      " 11  deck         203 non-null    category\n",
      " 12  embark_town  889 non-null    object  \n",
      " 13  alive        891 non-null    object  \n",
      " 14  alone        891 non-null    bool    \n",
      "dtypes: bool(2), category(2), float64(2), int64(4), object(5)\n",
      "memory usage: 80.7+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived\n",
      "0    549\n",
      "1    342\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_distribution = df['survived'].value_counts()\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values before cleaning: \n",
      "survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "age            177\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         2\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "deck           688\n",
      "embark_town      2\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n",
      "\n",
      "survived\n",
      "0    424\n",
      "1    288\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male  embark_town alive  alone  \n",
       "0    man        True  Southampton    no  False  \n",
       "1  woman       False    Cherbourg   yes  False  \n",
       "2  woman       False  Southampton   yes   True  \n",
       "3  woman       False  Southampton   yes  False  \n",
       "4    man        True  Southampton    no   True  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(f\"Missing Values before cleaning: \\n{missing_values}\\n\")\n",
    "\n",
    "# Remove 'deck' column since it has many missing values\n",
    "df.drop(columns=['deck'], inplace=True)\n",
    "\n",
    "# Remove rows with any missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Check new class distribution\n",
    "class_distribution = df['survived'].value_counts()\n",
    "print(class_distribution)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
    "y = df['survived']\n",
    "\n",
    "# Define preprocessing pipeline\n",
    "scaler = StandardScaler()\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', scaler, ['age', 'sibsp', 'parch', 'fare']),\n",
    "    ('cat', OneHotEncoder(drop='first'), ['pclass', 'sex', 'embarked'])\n",
    "])\n",
    "\n",
    "# Split dataset into training & test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply preprocessing to training & test data\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Practical Exercise: From Local to Decentralized Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q1** - Model Development and Deployment\n",
    "\n",
    "In this section, we will focus on building, evaluating, and deploying predictive models for the selected dataset. The key objectives are:\n",
    "\n",
    "- **Develop predictive models** : _Logistic Regression_, _Random Forest_ and _SVM_.\n",
    "- **Evaluate model accuracy and performance**: precision, recall, f1-score, accuracy.  \n",
    "- **Adapt the models for API access**: expose them via a Flask application with a GET `/predict` endpoint (`app.py`).\n",
    "- **Standardize the API response format**: ensure consistency across all models for seamless integration.\n",
    "```json\n",
    "    {\n",
    "        \"model\": \"<model_name>\",\n",
    "        \"input_features\": {\n",
    "            \"pclass\": 3,\n",
    "            \"sex\": \"male\",\n",
    "            \"age\": 22,\n",
    "            \"sibsp\": 1,\n",
    "            \"parch\": 0,\n",
    "            \"fare\": 7.25,\n",
    "            \"embarked\": \"S\"\n",
    "        },\n",
    "        \"prediction\": \"<Survived | Did not survive>\"\n",
    "    }\n",
    "```\n",
    "\n",
    "Additionally, the models deployed using Flask (`app.py`) can be tested through (`test.http`, section _Q1_). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.90      0.84        80\n",
      "           1       0.84      0.68      0.75        63\n",
      "\n",
      "    accuracy                           0.80       143\n",
      "   macro avg       0.81      0.79      0.80       143\n",
      "weighted avg       0.81      0.80      0.80       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict & evaluate\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
    "print(f'Logistic Regression Classification Report:\\n{classification_report(y_test, y_pred_logreg)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.84      0.81        80\n",
      "           1       0.78      0.71      0.74        63\n",
      "\n",
      "    accuracy                           0.78       143\n",
      "   macro avg       0.78      0.78      0.78       143\n",
      "weighted avg       0.78      0.78      0.78       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict & evaluate\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f'Random Forest Classification Report:\\n{classification_report(y_test, y_pred_rf)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.85      0.79        80\n",
      "           1       0.76      0.62      0.68        63\n",
      "\n",
      "    accuracy                           0.75       143\n",
      "   macro avg       0.75      0.73      0.74       143\n",
      "weighted avg       0.75      0.75      0.74       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train SVM model\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict & evaluate\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f'SVM Classification Report:\\n{classification_report(y_test, y_pred_svm)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/preprocessor.pkl']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save models & preprocessor\n",
    "joblib.dump(logreg, \"data/logreg_model.pkl\")\n",
    "joblib.dump(rf, \"data/rf_model.pkl\")\n",
    "joblib.dump(svm, \"data/svm_model.pkl\")\n",
    "joblib.dump(preprocessor, \"data/preprocessor.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q2** - Consensus Model Development\n",
    "\n",
    "In this section, we will focus on implementing a **consensus-based prediction model** that aggregates the outputs from multiple individual models to enhance reliability and robustness. The key objectives are:\n",
    "\n",
    "- **Develop a consensus model** by averaging predictions from _Logistic Regression_, _Random Forest_, and _SVM_.  \n",
    "- **Expose the consensus model via API** using a Flask `/predict/consensus` endpoint (`app.py`).  \n",
    "- **Evaluate performance** of the consensus model.\n",
    "- **Ensure inter-computer connectivity** using **ngrok**, allowing external systems to access the prediction service.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center;\">\n",
    "    <img src=\"../images/ngrok-activation.png\" alt=\"NGROK Activation\" width=\"500\">\n",
    "    <img src=\"../images/ngrok-consensus.png\" alt=\"NGROK Consensus\" width=\"500\">\n",
    "</div>\n",
    "<br>  \n",
    "\n",
    "To maintain consistency with individual model predictions, the **consensus model API** response format follows the same structured response:\n",
    "```json\n",
    "{\n",
    "    \"model\": \"consensus\",\n",
    "    \"input_features\": {\n",
    "        \"pclass\": 3,\n",
    "        \"sex\": \"male\",\n",
    "        \"age\": 22,\n",
    "        \"sibsp\": 1,\n",
    "        \"parch\": 0,\n",
    "        \"fare\": 7.25,\n",
    "        \"embarked\": \"S\"\n",
    "    },\n",
    "    \"individual_predictions\": {\n",
    "        \"logistic_regression\": \"Did not survive\",\n",
    "        \"random_forest\": \"Survived\",\n",
    "        \"svm\": \"Did not survive\"\n",
    "    },\n",
    "    \"final_prediction\": \"<Survived | Did not survive>\"\n",
    "}\n",
    "```\n",
    "The **final consensus prediction** is determined by averaging the outputs of all models and rounding the result. <br>\n",
    "The model can be tested through (`test.http`, section _Q2_) or through **ngrok** if the local server is running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consensus Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.90      0.83        80\n",
      "           1       0.84      0.65      0.73        63\n",
      "\n",
      "    accuracy                           0.79       143\n",
      "   macro avg       0.80      0.78      0.78       143\n",
      "weighted avg       0.80      0.79      0.79       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions for the test set using all models\n",
    "individual_preds = {model: model.predict(X_test) for model in [logreg, rf, svm]}\n",
    "\n",
    "# Compute consensus predictions\n",
    "consensus_preds = np.round(np.mean(np.array(list(individual_preds.values())), axis=0))\n",
    "\n",
    "# Evaluate accuracy\n",
    "consensus_accuracy = accuracy_score(y_test, consensus_preds)\n",
    "print(f\"Consensus Model Classification Report:\\n{classification_report(y_test,consensus_preds)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introducing Consensus with Slashing Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q3** - Weighted Consensus Model\n",
    "\n",
    "In this section, we enhance the consensus model by introducing a **dynamic weighting** system to refine predictions based on each model's performance over time. Instead of treating all models equally, we adjust their influence on the final prediction using dynamically updated weights. <br>\n",
    "The key objectives are:\n",
    "\n",
    "- Implement a **weighting system** where model contributions are adjusted based on accuracy.\n",
    "- Weights range from 0 to 1, reflecting each **model's reliability** in past predictions.\n",
    "- Weights are updated per **batch**, refining consensus predictions as more data is processed.\n",
    "\n",
    "The model can be tested through (`test.http`, section _Q3_) or through **ngrok** if the local server is running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Weights saved to data/model_weights.json: {'logistic_regression': 0.34943741580559107, 'random_forest': 0.33707891990152916, 'svm': 0.31348366429287977}\n",
      "\n",
      "Final Weighted Consensus Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.90      0.83        80\n",
      "           1       0.84      0.65      0.73        63\n",
      "\n",
      "    accuracy                           0.79       143\n",
      "   macro avg       0.80      0.78      0.78       143\n",
      "weighted avg       0.80      0.79      0.79       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Compute initial model accuracies\n",
    "model_accuracies = {\n",
    "    \"logistic_regression\": accuracy_score(y_test, y_pred_logreg),\n",
    "    \"random_forest\": accuracy_score(y_test, y_pred_rf),\n",
    "    \"svm\": accuracy_score(y_test, y_pred_svm)\n",
    "}\n",
    "\n",
    "# Normalize initial weights to sum to 1\n",
    "weights = np.array(list(model_accuracies.values()))\n",
    "weights /= weights.sum()  \n",
    "\n",
    "# Function to dynamically update weights\n",
    "def update_weights(y_true, predictions, weights, learning_rate=0.1):\n",
    "    predictions = np.array(predictions)  \n",
    "    y_true = np.array(y_true).flatten() \n",
    "\n",
    "    # Compute per-model accuracy in this batch\n",
    "    correct_predictions = (predictions.T == y_true).T  \n",
    "    accuracy_adjustments = correct_predictions.mean(axis=0)  \n",
    "    # Apply weighted adjustment\n",
    "    weights = weights * (1 + learning_rate * (accuracy_adjustments - weights))\n",
    "\n",
    "    # Normalize weights to sum to 1\n",
    "    weights /= weights.sum()\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Simulate multiple batches of predictions\n",
    "batch_size = 10\n",
    "num_batches = len(y_test) // batch_size\n",
    "pred_array = np.column_stack([preds.reshape(-1, 1) for preds in individual_preds.values()])\n",
    "\n",
    "for i in range(num_batches):\n",
    "    batch_start = i * batch_size\n",
    "    batch_end = batch_start + batch_size\n",
    "\n",
    "    # Get batch data\n",
    "    batch_y = y_test.iloc[batch_start:batch_end].values  \n",
    "    batch_preds = pred_array[batch_start:batch_end]  \n",
    "\n",
    "    # Compute weighted consensus prediction\n",
    "    weighted_preds = np.round(np.average(batch_preds, axis=1, weights=weights)).astype(int)\n",
    "\n",
    "    # Update weights based on batch performance\n",
    "    weights = update_weights(batch_y, batch_preds, weights)\n",
    "\n",
    "\n",
    "weights_dict = dict(zip([\"logistic_regression\", \"random_forest\", \"svm\"], weights.tolist()))\n",
    "with open(\"data/model_weights.json\", \"w\") as f:\n",
    "    json.dump(weights_dict, f)\n",
    "\n",
    "print(f\"\\nFinal Weights saved to data/model_weights.json: {weights_dict}\")\n",
    "print(f\"\\nFinal Weighted Consensus Classification Report:\\n{classification_report(y_test, np.round(np.average(pred_array, axis=1, weights=weights)).astype(int))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q4** - Proof-of-Stake Consensus & Slashing Mechanism\n",
    "\n",
    "In this section, we develop a **proof-of-stake (PoS)** consensus mechanism combined with a slashing protocol to ensure accountability and trust in the decentralized prediction system. Each model stakes an initial deposit (1000 euros) upon registration to participate in the prediction network. This deposit serves as a security measure, ensuring that models contribute accurate and reliable predictions.\n",
    "The key objectives are:\n",
    "\n",
    "- **Stake-Based Participation**: Models must provide an initial deposit to be eligible for participation.\n",
    "- **Penalty (Slashing) System**: Models that consistently make inaccurate predictions will have their stake reduced.\n",
    "- **Performance-Based Adjustments**: Weights are adjusted not only based on accuracy but also on penalties for unreliable models.\n",
    "- **Encouraging Trustworthiness**: Reliable models maintain or grow their stake, while inaccurate models risk financial loss.\n",
    "\n",
    "This system discourages malicious or low-quality contributions and promotes accuracy-driven participation. The model balance and slashing mechanism will are tracked in a local JSON database to simplify implementation.\n",
    "\n",
    "The model can be tested through (`test.http`, section Q4) or through **ngrok** if the local server is running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q4.1 Initialize & Load Balances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model balances initialized and saved: {'logistic_regression': 1000, 'random_forest': 1000, 'svm': 1000}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "BALANCE_FILE = \"data/model_balances.json\"\n",
    "INITIAL_BALANCE = 1000  # Initial deposit for each model\n",
    "SLASH_PENALTY = 50  # Penalty for poor performance (slashing)\n",
    "SLASHING_THRESHOLD = 0.3  # Accuracy threshold for slashing\n",
    "models = [\"logistic_regression\", \"random_forest\", \"svm\"]\n",
    "\n",
    "def initialize_and_save_balances(y_test, individual_preds):\n",
    "    \"\"\"\n",
    "    Initializes model balances, updates them based on accuracy, and saves to JSON.\n",
    "    \n",
    "    - Models start with an initial deposit.\n",
    "    - If accuracy is below the threshold, a slashing penalty is applied.\n",
    "    - Balances are saved to 'data/model_balances.json'.\n",
    "    \"\"\"\n",
    "    # Load existing balances if they exist, otherwise initialize them\n",
    "    if os.path.exists(BALANCE_FILE):\n",
    "        with open(BALANCE_FILE, \"r\") as f:\n",
    "            balances = json.load(f)\n",
    "    else:\n",
    "        balances = {model: INITIAL_BALANCE for model in models}\n",
    "\n",
    "    # Apply slashing for models below accuracy threshold\n",
    "    for model_name, accuracy in model_accuracies.items():\n",
    "        if accuracy < SLASHING_THRESHOLD:\n",
    "            balances[model_name] = max(0, balances[model_name] - SLASH_PENALTY)  # Ensure balance doesn't go negative\n",
    "\n",
    "    # Save updated balances to JSON\n",
    "    with open(BALANCE_FILE, \"w\") as f:\n",
    "        json.dump(balances, f)\n",
    "\n",
    "    print(f\"\\nModel balances initialized and saved: {balances}\")\n",
    "\n",
    "# Call function to initialize and save balances\n",
    "initialize_and_save_balances(y_test, individual_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q4.2 Load Balances for Use in Weighted Consensus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_balances():\n",
    "    \"\"\"Load model balances from JSON file or initialize if missing.\"\"\"\n",
    "    if os.path.exists(BALANCE_FILE):\n",
    "        with open(BALANCE_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        # If missing, reinitialize\n",
    "        return {model: INITIAL_BALANCE for model in models}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q4.3 Weighted Consensus with Proof-of-Stake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_with_slashing(y_true, predictions, weights, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Adjust weights dynamically after each batch based on accuracy.\n",
    "    Apply slashing if a model's accuracy falls below a threshold.\n",
    "    \"\"\"\n",
    "    predictions = np.array(predictions)  # Shape: (batch_size, num_models)\n",
    "    y_true = np.array(y_true).flatten()  # Shape: (batch_size,)\n",
    "\n",
    "    correct_predictions = (predictions.T == y_true).T\n",
    "    accuracy_adjustments = correct_predictions.mean(axis=0)  # Compute accuracy per model\n",
    "\n",
    "    # Load current balances\n",
    "    balances = load_balances()\n",
    "\n",
    "    # Adjust weights using a combination of performance and stake\n",
    "    for i, model_name in enumerate(models):\n",
    "        if accuracy_adjustments[i] < SLASHING_THRESHOLD:\n",
    "            balances[model_name] = max(0, balances[model_name] - SLASH_PENALTY)  # Apply slashing\n",
    "\n",
    "        # Use stake as a weight factor\n",
    "        weights[i] = balances[model_name] * (1 + learning_rate * (accuracy_adjustments[i] - weights[i]))\n",
    "\n",
    "    # Normalize weights\n",
    "    weights /= weights.sum()\n",
    "\n",
    "    # Save updated balances\n",
    "    with open(BALANCE_FILE, \"w\") as f:\n",
    "        json.dump(balances, f)\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q4.4 Apply PoS Consensus Over Multiple Batches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 1: Updated Weights = [0.33438819 0.33122363 0.33438819]\n",
      "\n",
      "Batch 2: Updated Weights = [0.33651805 0.33014862 0.33333333]\n",
      "\n",
      "Batch 3: Updated Weights = [0.33438819 0.33438819 0.33122363]\n",
      "\n",
      "Batch 4: Updated Weights = [0.3343983  0.33120341 0.3343983 ]\n",
      "\n",
      "Batch 5: Updated Weights = [0.33654877 0.33333333 0.3301179 ]\n",
      "\n",
      "Batch 6: Updated Weights = [0.3344086 0.3344086 0.3311828]\n",
      "\n",
      "Batch 7: Updated Weights = [0.33017876 0.33648791 0.33333333]\n",
      "\n",
      "Batch 8: Updated Weights = [0.33333333 0.33333333 0.33333333]\n",
      "\n",
      "Batch 9: Updated Weights = [0.33226496 0.33547009 0.33226496]\n",
      "\n",
      "Batch 10: Updated Weights = [0.33438819 0.33438819 0.33122363]\n",
      "\n",
      "Batch 11: Updated Weights = [0.33333333 0.33648791 0.33017876]\n",
      "\n",
      "Batch 12: Updated Weights = [0.33658009 0.33008658 0.33333333]\n",
      "\n",
      "Batch 13: Updated Weights = [0.33547009 0.33226496 0.33226496]\n",
      "\n",
      "Batch 14: Updated Weights = [0.33227513 0.33544974 0.33227513]\n"
     ]
    }
   ],
   "source": [
    "# Simulate multiple batches of predictions\n",
    "batch_size = 10\n",
    "num_batches = len(y_test) // batch_size\n",
    "pred_array = np.column_stack([\n",
    "    preds.reshape(-1, 1) for preds in individual_preds.values()\n",
    "])\n",
    "\n",
    "for i in range(num_batches):\n",
    "    batch_start = i * batch_size\n",
    "    batch_end = batch_start + batch_size\n",
    "\n",
    "    # Get batch data\n",
    "    batch_y = y_test.iloc[batch_start:batch_end].values  # Convert to NumPy array\n",
    "    batch_preds = pred_array[batch_start:batch_end]  # Shape: (batch_size, num_models)\n",
    "\n",
    "    # Compute proof-of-stake weighted consensus prediction\n",
    "    balances = load_balances()\n",
    "    pos_weights = np.array([balances[m] for m in models], dtype=float)  # Use stake as weight and ensure float type\n",
    "    pos_weights /= pos_weights.sum()  # Normalize\n",
    "\n",
    "    weighted_preds = np.round(np.average(batch_preds, axis=1, weights=pos_weights)).astype(int)\n",
    "\n",
    "    # Update weights with slashing\n",
    "    pos_weights = update_weights_with_slashing(batch_y, batch_preds, pos_weights)\n",
    "\n",
    "    print(f\"\\nBatch {i+1}: Updated Weights = {pos_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q4.5 Evaluate Accuracy and Performances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33227513 0.33544974 0.33227513]\n",
      "\n",
      "Final Weights saved to data/model_weights.json: {'logistic_regression': 0.3322751322751323, 'random_forest': 0.3354497354497355, 'svm': 0.3322751322751323}\n",
      "\n",
      "Final Weighted PoS Consensus Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.90      0.83        80\n",
      "           1       0.84      0.65      0.73        63\n",
      "\n",
      "    accuracy                           0.79       143\n",
      "   macro avg       0.80      0.78      0.78       143\n",
      "weighted avg       0.80      0.79      0.79       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights_dict = dict(zip([\"logistic_regression\", \"random_forest\", \"svm\"], pos_weights.tolist()))\n",
    "with open(\"data/pos_model_weights.json\", \"w\") as f:\n",
    "    json.dump(weights_dict, f)\n",
    "\n",
    "print(f\"\\nFinal Weights saved to data/model_weights.json: {weights_dict}\")\n",
    "print(f\"\\nFinal Weighted PoS Consensus Classification Report:\\n{classification_report(y_test, np.round(np.average(pred_array, axis=1, weights=final_pos_weights)).astype(int))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
